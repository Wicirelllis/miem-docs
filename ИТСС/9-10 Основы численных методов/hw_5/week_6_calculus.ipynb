{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. One-sided finite differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, `deriv`, which computes a derivative of its argument at a given point, $x$, using a one-sided finite difference rule with a given step side $h$, with the approximation order of $O(h^2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful link with coefficients\n",
    "# https://en.wikipedia.org/wiki/Finite_difference_coefficient\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def deriv(f, x, h):\n",
    "    \"\"\" Compute a derivative of `f` at point `x` with step size `h`.\n",
    "\n",
    "    Compute the derivative using the one-sided rule of the approximation order of $O(h^2)$.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The function to differentiate\n",
    "    x : float\n",
    "        The point to compute the derivative at.\n",
    "    h : float\n",
    "        The step size for the finite different rule.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fder : derivative of f(x) at point x using the step size h.\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "    # return (f(x + 2 * h) - f(x + h)) / h\n",
    "\n",
    "def deriv2(f, x, h):\n",
    "    \"\"\"Derivative based on two-point one-sided rule\"\"\"\n",
    "    return (-3/2 * f(x) + 2 * f(x + h) - 1/2 * f(x + 2*h)) / h\n",
    "\n",
    "def deriv3(f, x, h):\n",
    "    \"\"\"Derivative based on three-point one-sided rule\"\"\"\n",
    "    return (-11/6 * f(x) + 3 * f(x + h) - 3/2 * f(x + 2*h) + 1/3 * f(x + 3*h)) / h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test I.1\n",
    "\n",
    "Test your function on a simple test case: differentiate $f(x) = x^3$ at $x=0$. Comment on whether your results are consistent with the expected value of $f'(x) = 0$ and on an expected scaling with $h\\to 0$.\n",
    "\n",
    " (10% of the total grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000 --  0.0001\n",
      "0.001000 --   1e-06\n",
      "0.000100 --   1e-08\n",
      "0.000010 --   1e-10\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for h in [1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    err = deriv(lambda x: x**3, x, h)\n",
    "    print(\"%5f -- %7.4g\" % (h, err))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Everything works as expected. $O(h^2)$ indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test I.2\n",
    "\n",
    "Now use a slightly more complicated function, $f(x) = x^2 \\log{x}$, evaluate the derivative at $x=1$ using your one-sided rule and a two-point one-sided rule. Roughly estimate the value of $h$ where the error stops decreasing, for these two schemes. \n",
    "(15% of the total grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def f(x):\n",
    "    return x**2 * log(x)\n",
    "\n",
    "def fder(x):\n",
    "    return x * (2.*log(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x              error\n",
      "      1-point\n",
      "1.00000e-02,   1.50333e-02\n",
      "1.00000e-03,   1.50033e-03\n",
      "1.00000e-04,   1.50003e-04\n",
      "1.00000e-05,   1.50000e-05\n",
      "1.00000e-06,   1.49992e-06\n",
      "1.00000e-07,   1.50584e-07\n",
      "1.00000e-08,   8.92253e-09\n",
      "1.00000e-09,   8.42404e-08\n",
      "1.00000e-10,   8.28904e-08\n",
      "1.00000e-11,   8.27554e-08\n",
      "1.00000e-12,   8.89006e-05\n",
      "1.00000e-13,   -7.99278e-04\n",
      "1.00000e-14,   -7.99278e-04\n",
      "1.00000e-15,   1.10223e-01\n",
      "      2-point\n",
      "1.00000e-02,   -6.61713e-05\n",
      "1.00000e-03,   -6.66167e-07\n",
      "1.00000e-04,   -6.66628e-09\n",
      "1.00000e-05,   -4.90123e-11\n",
      "1.00000e-06,   -1.93956e-10\n",
      "1.00000e-07,   1.69408e-09\n",
      "1.00000e-08,   -1.71797e-08\n",
      "1.00000e-09,   1.93763e-07\n",
      "1.00000e-10,   8.27404e-08\n",
      "1.00000e-11,   8.27404e-08\n",
      "1.00000e-12,   1.99923e-04\n",
      "1.00000e-13,   -1.90950e-03\n",
      "1.00000e-14,   -7.99278e-04\n",
      "1.00000e-15,   2.21245e-01\n",
      "      3-point\n",
      "1.00000e-02,   -4.88245e-07\n",
      "1.00000e-03,   -4.99173e-10\n",
      "1.00000e-04,   -6.10290e-13\n",
      "1.00000e-05,   3.24567e-11\n",
      "1.00000e-06,   -3.41319e-10\n",
      "1.00000e-07,   3.17439e-09\n",
      "1.00000e-08,   -3.19827e-08\n",
      "1.00000e-09,   3.41792e-07\n",
      "1.00000e-10,   8.27404e-08\n",
      "1.00000e-11,   8.27404e-08\n",
      "1.00000e-12,   3.47953e-04\n",
      "1.00000e-13,   -3.38980e-03\n",
      "1.00000e-14,   -7.99278e-04\n",
      "1.00000e-15,   3.69275e-01\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "print(f\"  x              error\")\n",
    "print(f\"      1-point\")\n",
    "for h in np.logspace(-2, -15, 14):\n",
    "    err = deriv(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")\n",
    "\n",
    "print(f\"      2-point\")\n",
    "for h in np.logspace(-2, -15, 14):\n",
    "    err = deriv2(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")\n",
    "\n",
    "print(f\"      3-point\")\n",
    "for h in np.logspace(-2, -15, 14):\n",
    "    err = deriv3(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1-sided optimal $h \\approx 10^{-8}$.\n",
    "For 2-sided optimal $h \\approx 10^{-5}$.\n",
    "As we can see error is an order of magnitude $h$ for 1-point and $h^2$ for 2-point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test I.3 \n",
    "\n",
    "Now try differentiating $x^2 \\log(x)$ at $x=0$. Use the three-point one-sided rule. Note that to evaluate the function at zero, you need to special-case this value. Check the scaling of the error with $h$, explain your results. \n",
    "(25% of the total grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x              error\n",
      "      1-point\n",
      "1.00000e-17,   -3.91439e-16\n",
      "1.00000e-18,   -4.14465e-17\n",
      "1.00000e-19,   -4.37491e-18\n",
      "1.00000e-20,   -4.60517e-19\n",
      "1.00000e-21,   -4.83543e-20\n",
      "1.00000e-22,   -5.06569e-21\n",
      "1.00000e-23,   -5.29595e-22\n",
      "1.00000e-24,   -5.52620e-23\n",
      "1.00000e-25,   -5.75646e-24\n",
      "1.00000e-26,   -5.98672e-25\n",
      "1.00000e-27,   -6.21698e-26\n",
      "1.00000e-28,   -6.44724e-27\n",
      "1.00000e-29,   -6.67750e-28\n",
      "1.00000e-30,   -6.90776e-29\n",
      "      2-point\n",
      "1.00000e-17,   -1.38629e-17\n",
      "1.00000e-18,   -1.38629e-18\n",
      "1.00000e-19,   -1.38629e-19\n",
      "1.00000e-20,   -1.38629e-20\n",
      "1.00000e-21,   -1.38629e-21\n",
      "1.00000e-22,   -1.38629e-22\n",
      "1.00000e-23,   -1.38629e-23\n",
      "1.00000e-24,   -1.38629e-24\n",
      "1.00000e-25,   -1.38629e-25\n",
      "1.00000e-26,   -1.38629e-26\n",
      "1.00000e-27,   -1.38629e-27\n",
      "1.00000e-28,   -1.38629e-28\n",
      "1.00000e-29,   -1.38629e-29\n",
      "1.00000e-30,   -1.38629e-30\n",
      "      3-point\n",
      "1.00000e-17,   -8.63046e-18\n",
      "1.00000e-18,   -8.63046e-19\n",
      "1.00000e-19,   -8.63046e-20\n",
      "1.00000e-20,   -8.63046e-21\n",
      "1.00000e-21,   -8.63046e-22\n",
      "1.00000e-22,   -8.63046e-23\n",
      "1.00000e-23,   -8.63046e-24\n",
      "1.00000e-24,   -8.63046e-25\n",
      "1.00000e-25,   -8.63046e-26\n",
      "1.00000e-26,   -8.63046e-27\n",
      "1.00000e-27,   -8.63046e-28\n",
      "1.00000e-28,   -8.63046e-29\n",
      "1.00000e-29,   -8.63046e-30\n",
      "1.00000e-30,   -8.63046e-31\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x == 0:\n",
    "        # the limit of $x^2 log(x)$ at $x-> 0$ is zero, even though log(x) is undefined at x=0\n",
    "        return 0.0\n",
    "    else:\n",
    "        return x**2 * log(x)\n",
    "\n",
    "def fder(x):\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return x*(2*log(x) + 1)\n",
    "\n",
    "x = 0\n",
    "print(f\"  x              error\")\n",
    "print(f\"      1-point\")\n",
    "for h in np.logspace(-17, -30, 14):\n",
    "    err = deriv(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")\n",
    "\n",
    "print(f\"      2-point\")\n",
    "for h in np.logspace(-17, -30, 14):\n",
    "    err = deriv2(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")\n",
    "\n",
    "print(f\"      3-point\")\n",
    "for h in np.logspace(-17, -30, 14):\n",
    "    err = deriv3(f, x, h) - fder(x)\n",
    "    print(f\"{h:.5e},   {err:.5e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idk, error seems to be proportional to $h$. So probably it's a carefully constructed example to demonstrate something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Midpoint rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which computes a definite integral using the midpoint rule up to a given error, $\\epsilon$. Estimate the error by comparing the estimates of the integral at $N$ and $2N$ elementary intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def midpoint_rule(func, a, b, eps):\n",
    "    \"\"\" Calculate the integral of f from a to b using the midpoint rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function to integrate.\n",
    "    a : float\n",
    "        The lower limit of integration.\n",
    "    b : float\n",
    "        The upper limit of integration.\n",
    "    eps : float\n",
    "        The target accuracy of the estimate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    integral : float\n",
    "        The estimate of $\\int_a^b f(x) dx$.\n",
    "    \"\"\"\n",
    "    n = 1\n",
    "    while True:\n",
    "        if np.abs(\n",
    "            np.sum(f(np.linspace(a, b, n))) * (b - a) / n -\n",
    "            np.sum(f(np.linspace(a, b, 2 * n))) * (b - a) / (2 * n)) < eps:\n",
    "            return np.sum(f(np.linspace(a, b, n))) * (b - a) / n, n\n",
    "        else:\n",
    "            n = n * 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test II.1\n",
    "\n",
    "Test your midpoint rule on a simple integral, which you can calculate by paper and pencil.\n",
    "\n",
    "Compare the rate of convergence to the expected $O(N^{-2})$ scaling by studying the number of intervals required for a given accuracy $\\epsilon$.\n",
    "\n",
    "Compare the numerical results to the value you calculated by hand. Does the deviation agree with your estimate of the numerical error?\n",
    "(20% of the total grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e          integral    n\n",
      "1.000e-01  0.3888889  4\n",
      "1.000e-02  0.3444444  16\n",
      "1.000e-03  0.3346457  128\n",
      "1.000e-04  0.3334963  1024\n",
      "1.000e-05  0.3333435  16384\n",
      "1.000e-06  0.3333346  131072\n",
      "1.000e-07  0.3333335  1048576\n",
      "1.000e-08  0.3333334  8388608\n",
      "1.000e-09  0.3333333  134217728\n"
     ]
    }
   ],
   "source": [
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    return x ** 2\n",
    "\n",
    "print(f\" e          integral    n\")\n",
    "for e in np.logspace(-1, -9, 9):\n",
    "    i = midpoint_rule(f, 0, 1, e)\n",
    "    print(f\"{e:.3e}  {i[0]:.7}  {i[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence rate looks a bit off, but everything else is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test II.2\n",
    "\n",
    "Now use your midpoint rule to compute the value of\n",
    "\n",
    "$$\n",
    "\\int_0^1\\! \\frac{\\sin{\\sqrt{x}}}{x}\\, dx\n",
    "$$\n",
    "\n",
    "up to a predefined accuracy of $\\epsilon=10^{-4}$.\n",
    "\n",
    "Note that the integral contains an integrable singularity at the lower limit. Do calculations two ways: first, do a straightforward computation; next, subtract the singularity. Compare the number of iterations required to achieve the accuracy of $\\epsilon$.\n",
    "\n",
    "(30% of the total grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8290339213873215, 131072)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# straightforward calculation\n",
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sin(np.sqrt(x)) / x\n",
    "\n",
    "l = np.linspace(0.001, 1, 1_000_000)\n",
    "midpoint_rule(f, 0.001, 1, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.10772574272900005, 256)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtract the singularity\n",
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sin(np.sqrt(x)) / x - 1 / np.sqrt(x)\n",
    "\n",
    "l = np.linspace(0.0001, 1, 1_000_000)\n",
    "midpoint_rule(f, 0.001, 1, 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One need to add $$\\int_0^1 \\frac{1}{\\sqrt{x}} dx = 2$$ to second method to get the final value.\n",
    "\n",
    "As we can see subtract the singularity drastically reduces number of intervals for same precision.\n",
    "Wolframalpha says the integral is $1.89217$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4a30f445a6abfa47c23fb7a24a4ee9bfb1063e8d1c9c3b76c91bc697bd1ef32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
